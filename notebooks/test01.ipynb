{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize recognizer\n",
    "recognizer = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Projects\\\\Exp-speech-to-text\\\\notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import wave\n",
    "import os\n",
    "\n",
    "def record_and_save_mp3(output_filename=\"recording.mp3\"):\n",
    "    # Initialize recognizer\n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    # Record audio\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening Started...\")\n",
    "        audio = recognizer.listen(source)\n",
    "        print(\"Listening Completed...\")\n",
    "        \n",
    "        # First save as WAV\n",
    "        wav_filename = \"../data/processed/temp.wav\"\n",
    "        with wave.open(wav_filename, 'wb') as wf:\n",
    "            wf.setnchannels(1)  # Mono\n",
    "            wf.setsampwidth(audio.sample_width)\n",
    "            wf.setframerate(audio.sample_rate)\n",
    "            wf.writeframes(audio.get_raw_data())\n",
    "        \n",
    "        # Convert WAV to MP3\n",
    "        audio_segment = AudioSegment.from_wav(wav_filename)\n",
    "        audio_segment.export(output_filename, format=\"mp3\")\n",
    "        \n",
    "        # Remove temporary WAV file\n",
    "        # os.remove(wav_filename)\n",
    "        \n",
    "        print(f\"Audio saved as: {output_filename}\")\n",
    "        return output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening Started...\n",
      "Listening Completed...\n",
      "Audio saved as: ../data/processed/hey_boi.mp3\n"
     ]
    }
   ],
   "source": [
    "filename = record_and_save_mp3(\"../data/processed/hey_boi.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening Started...\n"
     ]
    }
   ],
   "source": [
    "with sr.Microphone() as source:\n",
    "    print(\"Listening Started...\")\n",
    "    audio = recognizer.listen(source)  # Listen for audio input\n",
    "    print(\"Listening Completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<speech_recognition.audio.AudioData at 0x28682851450>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Web Speech API works online, so youâ€™ll need an internet connection to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognizing...\n",
      "You said: how many sales representative are present in\n"
     ]
    }
   ],
   "source": [
    "# Use Google Web Speech API\n",
    "try:\n",
    "    print(\"Recognizing...\")\n",
    "    text = recognizer.recognize_google(audio)\n",
    "    print(\"You said:\", text)\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Sorry, I could not understand your speech.\")\n",
    "except sr.RequestError as e:\n",
    "    print(\"Sorry, an error occurred. {0}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpeechRecognition with CMU Sphinx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized Text: all the tears in person to cuba and mighty\n"
     ]
    }
   ],
   "source": [
    "# Use CMU Sphinx for offline recognition\n",
    "try:\n",
    "    text = recognizer.recognize_sphinx(audio)\n",
    "    print(\"Recognized Text:\", text)\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Sorry, Sphinx could not understand the audio.\")\n",
    "except sr.RequestError as e:\n",
    "    print(f\"Sphinx error; {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying all recognizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_recognizers(audio_data):\n",
    "    recognizer = sr.Recognizer()\n",
    "    results = {}\n",
    "\n",
    "    # 1. Google Speech Recognition (Free)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        results['Google (Free)'] = text\n",
    "    except sr.RequestError as e:\n",
    "        results['Google (Free)'] = f\"Error: {str(e)}\"\n",
    "    except sr.UnknownValueError:\n",
    "        results['Google (Free)'] = \"Unable to recognize speech\"\n",
    "\n",
    "    # # 2. Google Cloud Speech (Paid - Requires API Key)\n",
    "    # try:\n",
    "    #     text = recognizer.recognize_google_cloud(\n",
    "    #         audio_data,\n",
    "    #         credentials_json='your-google-cloud-credentials.json',\n",
    "    #         language='en-US'\n",
    "    #     )\n",
    "    #     results['Google Cloud'] = text\n",
    "    # except sr.RequestError as e:\n",
    "    #     results['Google Cloud'] = f\"Error: {str(e)}\"\n",
    "    # except sr.UnknownValueError:\n",
    "    #     results['Google Cloud'] = \"Unable to recognize speech\"\n",
    "\n",
    "    # 3. Wit.ai (Free - Requires API Key)\n",
    "    try:\n",
    "        text = recognizer.recognize_wit(\n",
    "            audio_data,\n",
    "            key=\"LGUJMGK64BDJUGBUDAFPEXDICAAKVX7J\"\n",
    "        )\n",
    "        results['Wit.ai'] = text\n",
    "    except sr.RequestError as e:\n",
    "        results['Wit.ai'] = f\"Error: {str(e)}\"\n",
    "    except sr.UnknownValueError:\n",
    "        results['Wit.ai'] = \"Unable to recognize speech\"\n",
    "\n",
    "    # # 4. Microsoft Azure (Paid - Requires API Key)\n",
    "    # try:\n",
    "    #     text = recognizer.recognize_azure(\n",
    "    #         audio_data,\n",
    "    #         key=\"YOUR-AZURE-KEY\",\n",
    "    #         location=\"westus\",  # Azure region\n",
    "    #         language=\"en-US\"\n",
    "    #     )\n",
    "    #     results['Azure'] = text\n",
    "    # except sr.RequestError as e:\n",
    "    #     results['Azure'] = f\"Error: {str(e)}\"\n",
    "    # except sr.UnknownValueError:\n",
    "    #     results['Azure'] = \"Unable to recognize speech\"\n",
    "\n",
    "    # # 5. IBM Watson (Paid - Requires API Key)\n",
    "    # try:\n",
    "    #     text = recognizer.recognize_ibm(\n",
    "    #         audio_data,\n",
    "    #         username=\"IBM-USERNAME\",\n",
    "    #         password=\"IBM-PASSWORD\"\n",
    "    #     )\n",
    "    #     results['IBM Watson'] = text\n",
    "    # except sr.RequestError as e:\n",
    "    #     results['IBM Watson'] = f\"Error: {str(e)}\"\n",
    "    # except sr.UnknownValueError:\n",
    "    #     results['IBM Watson'] = \"Unable to recognize speech\"\n",
    "\n",
    "    # 6. Sphinx (Offline)\n",
    "    try:\n",
    "        text = recognizer.recognize_sphinx(audio_data)\n",
    "        results['Sphinx'] = text\n",
    "    except sr.RequestError as e:\n",
    "        results['Sphinx'] = f\"Error: {str(e)}\"\n",
    "    except sr.UnknownValueError:\n",
    "        results['Sphinx'] = \"Unable to recognize speech\"\n",
    "\n",
    "    # # 7. Houndify (Paid - Requires API Key)\n",
    "    # try:\n",
    "    #     text = recognizer.recognize_houndify(\n",
    "    #         audio_data,\n",
    "    #         client_id=\"HOUNDIFY-CLIENT-ID\",\n",
    "    #         client_key=\"HOUNDIFY-CLIENT-KEY\"\n",
    "    #     )\n",
    "    #     results['Houndify'] = text\n",
    "    # except sr.RequestError as e:\n",
    "    #     results['Houndify'] = f\"Error: {str(e)}\"\n",
    "    # except sr.UnknownValueError:\n",
    "    #     results['Houndify'] = \"Unable to recognize speech\"\n",
    "\n",
    "    # # 8. Amazon Transcribe (Paid - Requires AWS Credentials)\n",
    "    # try:\n",
    "    #     text = recognizer.recognize_amazon(\n",
    "    #         audio_data,\n",
    "    #         region=\"us-west-2\",\n",
    "    #         aws_access_key_id=\"AWS-ACCESS-KEY\",\n",
    "    #         aws_secret_access_key=\"AWS-SECRET-KEY\"\n",
    "    #     )\n",
    "    #     results['Amazon'] = text\n",
    "    # except sr.RequestError as e:\n",
    "    #     results['Amazon'] = f\"Error: {str(e)}\"\n",
    "    # except sr.UnknownValueError:\n",
    "    #     results['Amazon'] = \"Unable to recognize speech\"\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_and_transcribe():\n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Adjusting for ambient noise...\")\n",
    "        recognizer.adjust_for_ambient_noise(source, duration=2)\n",
    "        \n",
    "        print(\"Please speak something...\")\n",
    "        try:\n",
    "            audio = recognizer.listen(source, timeout=5, phrase_time_limit=15)\n",
    "            print(\"Processing audio with multiple services...\")\n",
    "            \n",
    "            results = try_all_recognizers(audio)\n",
    "            \n",
    "            print(\"\\nResults from different services:\")\n",
    "            print(\"-\" * 50)\n",
    "            for service, result in results.items():\n",
    "                print(f\"{service}: {result}\")\n",
    "                \n",
    "        except sr.WaitTimeoutError:\n",
    "            print(\"No speech detected within timeout period\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting for ambient noise...\n",
      "Please speak something...\n",
      "Processing audio with multiple services...\n",
      "\n",
      "Results from different services:\n",
      "--------------------------------------------------\n",
      "Google (Free): hello good morning everyone this is Mohammed jamdani reporting from the ground zero and testing accuracy of the models\n",
      "Wit.ai: Hello good morning everyone this is reporting on the ground oh testing accuration of the models\n",
      "Sphinx: an old one morning adorned the thermometer non if we're to come but don't you work at the liquid his orders\n"
     ]
    }
   ],
   "source": [
    "record_and_transcribe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "openai/whisper-large-v3-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.46.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying Safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "file_path = \"..\\src\\models\\model.safetensors\"\n",
    "loaded = load_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace OpenAI/Whisper large-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51866, 1280, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1280, out_features=51866, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\envs\\estt\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # audio_array = convert_audio_data(audio)\n",
    "    result = pipe(\"../data/raw/Rachel-Lind.wav\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing audio: {str(e)}\")\n",
    "    # Handle error appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CHAPTER I. Mrs. Rachel Lynde is Surprised. Mrs. Rachel Lynde lived just where the Avonlea main road dipped down into a little hollow, fringed with alders and ladies' ear-drops, and traversed by a brook,\n"
     ]
    }
   ],
   "source": [
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct from transformers Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers torch\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "whisper_pipeline = pipeline(\"automatic-speech-recognition\", \"openai/whisper-large-v3\", torch_dtype=torch_dtype, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\envs\\estt\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " chapter i mrs rachel lynde is surprised mrs rachel lynde lived just where the avonlea main road dipped down into a little hollow fringed with alders and ladies ear-drops and traversed by a brook\n"
     ]
    }
   ],
   "source": [
    "transcription = whisper_pipeline(\"../data/raw/Rachel-Lind.wav\")\n",
    "\n",
    "print(transcription[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using openai-whisper via pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from pydub.effects import normalize\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "import io\n",
    "import os\n",
    "import whisper\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_preprocess_audio(audio_file, export_path=None):\n",
    "    print(f\"Starting enhanced audio preprocessing for {audio_file}\")\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    \n",
    "    # Convert to mono\n",
    "    audio = audio.set_channels(1)\n",
    "    \n",
    "    # Normalize audio (adjust volume to a standard level)\n",
    "    audio = normalize(audio)\n",
    "    \n",
    "    # Increase volume by 10 dB (adjust as needed)\n",
    "    audio = audio + 10\n",
    "    \n",
    "    # Remove low frequencies (below 80Hz) which are often noise\n",
    "    audio = audio.high_pass_filter(80)\n",
    "    \n",
    "    # Set sample rate to 16kHz\n",
    "    audio = audio.set_frame_rate(16000)\n",
    "    \n",
    "    # If export_path is not provided, create one based on the input filename\n",
    "    if export_path is None:\n",
    "        base_name = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "        export_path = f\"preprocessed_{base_name}.wav\"\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    directory = os.path.dirname(export_path)\n",
    "    if directory and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "    # Export to file\n",
    "    audio.export(export_path, format=\"wav\")\n",
    "    print(f\"Preprocessed audio exported to {export_path}\")\n",
    "    \n",
    "    print(\"Enhanced audio preprocessing completed.\")\n",
    "    return export_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_with_whisper(audio_file_path, model_size=\"base\"):\n",
    "    \"\"\"\n",
    "    Transcribe audio using OpenAI's Whisper model.\n",
    "    \n",
    "    :param audio_file_path: Path to the audio file\n",
    "    :param model_size: Size of the Whisper model to use (\"tiny\", \"base\", \"small\", \"medium\", \"large\")\n",
    "    :return: Transcribed text\n",
    "    \"\"\"\n",
    "    # Check for CUDA availability\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the Whisper model\n",
    "    model = whisper.load_model(model_size).to(device)\n",
    "\n",
    "    # Perform the transcription\n",
    "    result = model.transcribe(audio=audio_file_path, language=\"en\")\n",
    "\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio):\n",
    "    print(\"Starting audio transcription...\")\n",
    "    try:\n",
    "        transcript = transcribe_with_whisper(audio)\n",
    "        print(\"Audio transcription completed.\")\n",
    "    \n",
    "    except sr.UnknownValueError as e:\n",
    "        print(\"Speech recognition could not understand audio\")\n",
    "        print(f\"Speech recognition could not understand audio: {e}\")\n",
    "        print(f\"Audio file: {audio}\")\n",
    "        print(f\"Audio duration: {len(AudioSegment.from_file(audio))/1000} seconds\")\n",
    "        return \"\"\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from speech recognition service; {e}\")\n",
    "        return \"\"\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(audio_file):\n",
    "\n",
    "    audio = enhanced_preprocess_audio(audio_file,\"../data/processed/temp.wav\")\n",
    "    transcript = transcribe_audio(audio)\n",
    "    if not transcript:\n",
    "        print(\"Transcription failed.\")\n",
    "        return ''\n",
    "    \n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the listening process for file: {audio_file}\n",
      "Starting enhanced audio preprocessing for ../data/processed/temp.wav\n",
      "Preprocessed audio exported to ../data/processed/temp.wav\n",
      "Enhanced audio preprocessing completed.\n",
      "Starting audio transcription...\n",
      "Using device: cpu\n",
      "Audio transcription completed.\n",
      "Lyric sync process completed.\n",
      "\n",
      " Hello Mr.T.E.J, my song is so pleasing.\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "\n",
    "audio_file = \"../data/processed/temp.wav\"\n",
    "print(\"Starting the listening process for file: {audio_file}\")\n",
    "transcript = run(audio_file)\n",
    "print(\"Lyric sync process completed.\")\n",
    "print()\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
